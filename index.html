<!doctype html>
<html>

<head>
<title>XIAOFAN LI</title>

<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="keywords" content="Xiaofan Li, 3D Computer Vision, Autonomous Driving, World Models"> 
<meta name="description" content="Xiaofan Li's home page">
<link rel="stylesheet" href="css/jemdoc.css" type="text/css" />

</head>


<body>

<div id="layout-content" style="margin-top:25px">

<table>
	<tbody>
		<tr>
			<td width="75%">
				<div id="toptitle">
					<h1>XIAOFAN LI<h1>
				</div>

				<p>
					I’m a graduate of Zhejiang University and currently a Technical Expert in the Autonomous Driving Foundation Model Department at Baidu. My work lies at the intersection of 3D computer vision, autonomous driving, and embodied intelligence, with a focus on open-set 3D perception and planning, vision-language models, implicit rendering and generation, and world models.
				</p>
				<p>
					<b>Research Interests:</b> Recently, my research has been centered on open-set 3D spatial perception and planning, as well as unified multimodal models for understanding, perception, planning, and generation.
				</p>
	            <p>
                    <b>Location</b>: Beijing, China | <b>Pronouns</b>: he/him/his </br>
				    <b>Email</b>: <a href="mailto:shalfunnn@gmail.com">shalfunnn@gmail.com</a> | <b>Tel</b>: <a href="tel:+86-18502600860">+86 18502600860</a> </br>
				    [<a href="https://github.com/shalfun/">Github</a>]
				</p>
			</td>
			<td width="25%">
				<!-- Please replace this with your own photo -->
				<img src="assets/imgs/xiaofan_li.jpg" width="100%"/>
			</td>
		</tr>
	</tbody>
</table>


<h2>Education</h2>
<p>
    <b>Zhejiang University(ZJU)</b>, BS in Optical Science and Engineering <br>
    <i>Sept 2015 – May 2019</i>
</p>
<ul>
    <li><b>Institute of Optical Engineering – Advisor: Kaiwei Wang (2017)</b><br>
        Undergraduate Research Training Program: One-shot face recognition assisted by visual information.
    </li>
    <li><b>CAD&CG – Advisors: Guofeng Zhang & SenseTime Internship (2018)</b><br>
        3D object motion estimation using stereo vision and optical flow prediction based on simulation data.
    </li>
    <li><b>6th National Optoelectronic Design Competition (2018)</b><br>
        Team leader of a project developing an autonomous fire-extinguishing vehicle based on light source detection. Awarded national second prize.
    </li>
    <li><b>Image Process Lab – Advisor: Zhihai Xu (2019)</b><br>
        Dynamic range extension using a dual-resolution camera system for the imaging system of the Chang’e-4 lunar mission.
    </li>
</ul>

<h2>Work Experience</h2>
<div>
    <p><b>Machine Learning Engineer & Research Scientist, Baidu</b> -- Beijing, CN <br>
    <i>Jul 2021 -- Present</i></p>
    <ul>
        <li>Responsible for preliminary research on unified framework(Diffusion & AR) for open-world understanding, perception, planning, and generation.</li>
        <li>Responsible for preliminary research on autonomous driving scenario generation and editing (Diffusion & 3DGS), as well as world model.</li>
        <li>Responsible for end-to-end autonomous driving localization.</li>
        <li>Responsible for early-stage BEV perception algorithm data pipeline and model development.</li>
    </ul>
</div>
<div>
    <p><b>Machine Learning Engineer, Rockchip</b> -- Hangzhou, CN <br>
    <i>Jun 2019 -- Jun 2021</i></p>
    <ul>
        <li>Responsible for face detection and face reconstruction on vivo smartphones.</li>
        <li>Responsible for RAW-domain denoising and other ISP-related tasks in Super Night Mode on vivo smartphones.</li>
        <li>Participated in the initial development of the RKNN open-source inference acceleration library.</li>
    </ul>
</div>

<h2>Skills</h2>
<p><b>Languages:</b> Mandarin(Native), English(CET-6)</p>
<p><b>Technologies:</b> C, C++, Shell, MATLAB, Python</p>


<h2>Publications</h2>
<!-- Please replace the image paths with your actual publication images -->
    <div class="paper" style="clear:left;">
      <div class="pimg" style="float:left;margin-bottom:10px;padding-left:3px;border-bottom:solid 1px #EEE;box-shadow:0 0px 2px">
          <img src="assets/imgs/paper1.jpg" width="200" >
      </div>
      <div class="ptitle" style="padding:1px;margin-left:215px;">
          DrivingDiffusion: Layout-Guided Multi-View Driving Scenarios Video Generation with Latent Diffusion Model
      </div>
      <div class="pauthors" style="padding:1px;margin-left:215px;">
          <b>Xiaofan Li</b>, Yifu Zhang, Xiaoqing Ye
      </div>
      <div class="pvenue" style="padding:1px;margin-left:215px;">
          European Conference on Computer Vision (<b>ECCV</b>), 2024
          <p>[<a href="#" target="_blank" rel="noopener">paper</a>] [<a href="#" target="_blank" rel="noopener">code</a>]</p>
      </div>
    </div>
    
    </br>
						
    <div class="paper" style="clear:left;">
      <div class="pimg" style="float:left;margin-bottom:10px;padding-left:3px;border-bottom:solid 1px #EEE;box-shadow:0 0px 2px">
          <img src="assets/imgs/paper2.jpg" width="200" >
      </div>
      <div class="ptitle" style="padding:1px;margin-left:215px;">
          MPDrive: Improving Spatial Understanding with Marker-Based Prompt Learning for Autonomous Driving
      </div>
      <div class="pauthors" style="padding:1px;margin-left:215px;">
          Zhiyuan Zhang*, <b>Xiaofan Li*</b>, Zhihao Xu, Wenjie Peng, Zijian Zhou, Miaojing Shi, Shuangping Huang
      </div>
      <div class="pvenue" style="padding:1px;margin-left:215px;">
          <b>[Highlight]</b> Computer Vision and Pattern Recognition Conference (<b>CVPR</b>), 2025
          <p>[<a href="#" target="_blank" rel="noopener">paper</a>] [<a href="#" target="_blank" rel="noopener">code</a>]</p>
      </div>
    </div>
    
    </br>

    <div class="paper" style="clear:left;">
      <div class="pimg" style="float:left;margin-bottom:10px;padding-left:3px;border-bottom:solid 1px #EEE;box-shadow:0 0px 2px">
          <img src="assets/imgs/paper3.jpg" width="200" >
      </div>
      <div class="ptitle" style="padding:1px;margin-left:215px;">
          U-ViLAR: Uncertainty-Aware Visual Localization for Autonomous Driving via Differentiable Association and Registration
      </div>
      <div class="pauthors" style="padding:1px;margin-left:215px;">
          <b>Xiaofan Li</b>, Zhihao Xu, Chenming Wu, Zhao Yang, Yumeng Zhang, Jiang-Jiang Liu, Haibao Yu, Xiaoqing Ye, YuAn Wang, Shirui Li, Xun Sun, Ji Wan, Jun Wang
      </div>
      <div class="pvenue" style="padding:1px;margin-left:215px;">
          International Conference on Computer Vision (<b>ICCV</b>), 2025
          <p>[<a href="#" target="_blank" rel="noopener">paper</a>] [<a href="#" target="_blank" rel="noopener">code</a>]</p>
      </div>
    </div>
    
    </br>
	
	<!-- Add other publications following the same structure... -->

    <div class="paper" style="clear:left;">
      <div class="pimg" style="float:left;margin-bottom:10px;padding-left:3px;border-bottom:solid 1px #EEE;box-shadow:0 0px 2px">
          <img src="assets/imgs/paper_driverse.jpg" width="200" >
      </div>
      <div class="ptitle" style="padding:1px;margin-left:215px;">
          DriVerse: Navigation World Model for Driving Simulation via Multimodal Trajectory Prompting and Motion Alignment
      </div>
      <div class="pauthors" style="padding:1px;margin-left:215px;">
          <b>Xiaofan Li</b>, Chenming Wu, Zhao Yang, Zhihao Xu, Dingkang Liang, Yumeng Zhang, Ji Wan, Jun Wang
      </div>
      <div class="pvenue" style="padding:1px;margin-left:215px;">
          arXiv preprint arXiv:2504.18576
           <p>[<a href="https://arxiv.org/abs/2504.18576" target="_blank" rel="noopener">paper</a>]</p>
      </div>
    </div>

    </br>

    <div class="paper" style="clear:left;">
      <div class="pimg" style="float:left;margin-bottom:10px;padding-left:3px;border-bottom:solid 1px #EEE;box-shadow:0 0px 2px">
          <img src="assets/imgs/paper_bevworld.jpg" width="200" >
      </div>
      <div class="ptitle" style="padding:1px;margin-left:215px;">
          BevWorld: A Multimodal World Model for Autonomous Driving via Unified BEV Latent Space
      </div>
      <div class="pauthors" style="padding:1px;margin-left:215px;">
          Yumeng Zhang*, Shi Gong*, Kaixin Xiong*, Xiaoqing Ye, <b>Xiaofan Li</b>, Xiao Tan, Fan Wang, Jizhou Huang, Hua Wu, Haifeng Wang
      </div>
      <div class="pvenue" style="padding:1px;margin-left:215px;">
          arXiv preprint arXiv:2407.05679
           <p>[<a href="https://arxiv.org/abs/2407.05679" target="_blank" rel="noopener">paper</a>]</p>
      </div>
    </div>
	
	</br>
	
	<h3>In Progress:</h3>
	<div class="paper" style="clear:left;">
      <div class="pimg" style="float:left;margin-bottom:10px;padding-left:3px;border-bottom:solid 1px #EEE;box-shadow:0 0px 2px">
          <img src="assets/imgs/paper_inprogress1.jpg" width="200" >
      </div>
      <div class="ptitle" style="padding:1px;margin-left:215px;">
          LoopGen: Generative Street Scene Expansion via Diffusion-Aided Outlier Repair
      </div>
      <div class="pauthors" style="padding:1px;margin-left:215px;">
          <b>Xiaofan Li</b>, Yuan Wang, Ji Wan, Jun Wang
      </div>
      <div class="pvenue" style="padding:1px;margin-left:215px;">
          In Progress, 2025
      </div>
    </div>
	
	<!-- Add other in-progress papers... -->

<h2>Projects</h2>

<div>
    <p>
        <b>Baidu | Machine Learning Engineer</b> <br>
        <i>Jul 2021 -- Present</i>
    </p>
    
    <div style="margin-left: 20px;">
        <p>
            <b>Foundation Model for 3D Perception and Planning | Tech Lead (Lead of 2)</b> <br>
            <i>Feb 2025 -- Present</i>
        </p>
        <ul>
            <li>Developed a unified language-aware perception and generation framework for open-world autonomous driving, supporting joint prediction of object detection, instance segmentation, relative depth estimation, and pixel-level tracking. Trained on business data with minimal calibration, the model achieved strong cross-domain generalization and robust performance in obstacle detection/tracking and occlusion flow estimation across diverse environments. Integrated vision-language modeling (VLM) to provide cognition-level guidance for downstream tasks such as intention prediction and trajectory planning, significantly improving decision-making and trajectory forecasting in complex dynamic scenes.</li>
        </ul>

        <p>
            <b>Scene-level Generation and World Model | Tech Lead (Lead of 2)</b> <br>
            <i>Jan 2023 -- Present</i>
        </p>
        <ul>
            <li><b>Scene Generation:</b> Pioneered controllable single-/multi-view image and video generation ahead of ControlNet. Developed the first framework for controllable large-scale multi-view video generation, extending T2I models with temporal modeling and introducing a localized prompt attention mechanism to enhance instance quality. Proposed a spatio-temporal cross-view autoregressive prediction framework for consistent multi-view synthesis. Built two production pipelines for generative data: one based on sparse layout control, the other on dense Occ/MPI representations (rule-based and stochastic), enabling structured video generation. Independently led and completed DrivingDiffusion: Layout-Guided Multi-View Driving Scenarios Video Generation with Latent Diffusion Model (ECCV 2024). The localized prompt attention mechanism was further extended for fine-grained region-level understanding in vision-language modeling, leading to <i>MPDrive: Improving Spatial Understanding with Marker-Based Prompt Learning for Autonomous Driving (CVPR 2025 Highlight)</i>.</li>
            <li><b>Implicit Rendering:</b> Built a dynamic-static street scene reconstruction and rendering system based on 3D Gaussian Splatting. Developed a complete simulation framework covering data acquisition, instance-level scene editing, and generative outlier completion from novel views. Implemented iterative refinement techniques for high-fidelity cross-lane rendering and occlusion-aware completion. Delivered over 150K high-quality training/testing samples, demonstrating performance gains in downstream perception tasks. This work was extended to the foundational domain and published as <i>Drag Your Gaussian: Effective Drag-Based Editing with Score Distillation for 3D Gaussian Splatting (SIGGRAPH 2025)</i>.</li>
            <li><b>World Model:</b> Designed and developed a multimodal world model framework for autonomous driving, where 3D scene representations are compressed into a unified latent space via VAE. The framework integrates a BEV perception module, VAE-based generative modeling, BEV decoding, and point cloud/image RGB rendering modules. Built Diffusion + Autoregressive models for both high-level agent behavior prediction and low-level observation forecasting, validated across public benchmarks and internal production data. The project led to the collaborative publication of <i>BevWorld: A Multimodal World Model for Autonomous Driving via Unified BEV Latent Space</i>.</li>
        </ul>
		
		<!-- Add other projects following the same structure... -->

    </div>

    <p>
        <b>RockChip Algo-ZJU | Machine Learning Engineer</b> <br>
        <i>Jun 2019 -- Jun 2021</i>
    </p>
    <div style="margin-left: 20px;">
        <ul>
            <li><b>RAW Denoising:</b> Designed lightweight RAW domain denoising models, optimized for block-edge artifacts and extreme low-light conditions. Improved Super Night algorithm pipeline, achieving lower DDR memory usage and surpassing competitors (e.g., SenseTime, ArcSoft) in performance. Deployed on vivo and Transsion devices.</li>
            <li><b>Inference Optimization:</b> Conducted multi-platform (CPU/GPU/DSP/NPU) performance tuning. Built quantized, efficient inference engines for DSP/NPU, achieving 52ms for 4K RAW on RK1608. Optimized front/back pipelines with Vec-C instruction-level parallelism, gaining 10x+ speedup. Contributed to the open-source RKNN inference toolkit with custom Depthwise/Group Conv ops. <b>GitHub</b>: <a href="https://github.com/airockchip/rknn-toolkit2">https://github.com/airockchip/rknn-toolkit2</a> (1.8k Stars)</li>
            <li><b>Scene Recognition & 3D Face Reconstruction:</b> Developed a real-time scene recognition system, improved mobile face detection, and implemented 3D face reconstruction based on monocular camera + IMU. Delivered to Vivo, Transsion, & BBK Electronics.</li>
        </ul>
    </div>
</div>


<table width="100%"> 
	<tr> 
		<td align="center">© XIAOFAN LI | Last updated in September 2024</td>
	</tr> 
</table>

</div>

</body>
</html>
